# -*- coding: utf-8 -*-
"""NFL_Bot_OpenMeteo_Starter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VOhoF7XQPJ08JcYYPMUcC48KnvdR5a72

# NFL Assistant Bootstrap (Colab) — Open‑Meteo Weather (Free, No Key)

This notebook wires up:
- **nfl_data_py** for historical data (weekly stats, injuries, depth charts, schedules, scoring lines, NGS summaries)
- **The Odds API v4** for **h2h**, **spreads**, **totals**
- **Open‑Meteo** weather (free, **no API key**) via `/v1/forecast` and **Open‑Meteo Geocoding** to resolve city → lat/lon
- `.env` loader from **/content/drive/MyDrive/NFL_Bot/.env** (must contain `ODDS_API_KEY`)

> On game day, check **inactives ~90 minutes before kickoff** (manually or via a paid feed) to finalize decisions.
"""

# === Install dependencies (Colab) ===
!pip -q install pandas numpy requests python-dotenv duckdb pyarrow tzdata
!pip install openai

# Commented out IPython magic to ensure Python compatibility.
# ===== 1) Install/upgrade =====
# %pip -q install -U nfl_data_py==0.3.1

# === Mount Google Drive and load .env ===
import os
from pathlib import Path

def _maybe_mount_drive():
    try:
        import google.colab  # type: ignore
        from google.colab import drive
        drive.mount('/content/drive', force_remount=False)
        return True
    except Exception:
        return False

_MOUNTED = _maybe_mount_drive()
ENV_PATH = '/content/drive/MyDrive/NFL_Bot/.env' if _MOUNTED else str(Path.cwd() / '.env')

from dotenv import load_dotenv
_ = load_dotenv(ENV_PATH)

print(f"Loaded .env from: {ENV_PATH}")
print("Has ODDS_API_KEY:", bool(os.getenv("ODDS_API_KEY")))
print("Has OPENAI_API_KEY:", bool(os.getenv("OPENAI_API_KEY")))  # do not print the value

if not os.getenv("ODDS_API_KEY"):
    raise RuntimeError("Missing ODDS_API_KEY in .env")

if not os.getenv("OPENAI_API_KEY"):
    raise RuntimeError("Missing OPENAI_API_KEY in .env")

# =========================
# 1) Imports & mappings
# =========================
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import requests
import datetime as dt
import hashlib, re
import nfl_data_py as nfl
from zoneinfo import ZoneInfo
from IPython.display import display

pd.set_option('display.max_columns', None)

TEAM_CODE_TO_FULL = {
    'ARI':'Arizona Cardinals','ATL':'Atlanta Falcons','BAL':'Baltimore Ravens','BUF':'Buffalo Bills',
    'CAR':'Carolina Panthers','CHI':'Chicago Bears','CIN':'Cincinnati Bengals','CLE':'Cleveland Browns',
    'DAL':'Dallas Cowboys','DEN':'Denver Broncos','DET':'Detroit Lions','GB':'Green Bay Packers',
    'HOU':'Houston Texans','IND':'Indianapolis Colts','JAX':'Jacksonville Jaguars','KC':'Kansas City Chiefs',
    'LAC':'Los Angeles Chargers','LAR':'Los Angeles Rams','LV':'Las Vegas Raiders','MIA':'Miami Dolphins',
    'MIN':'Minnesota Vikings','NE':'New England Patriots','NO':'New Orleans Saints','NYG':'New York Giants',
    'NYJ':'New York Jets','PHI':'Philadelphia Eagles','PIT':'Pittsburgh Steelers','SEA':'Seattle Seahawks',
    'SF':'San Francisco 49ers','TB':'Tampa Bay Buccaneers','TEN':'Tennessee Titans','WAS':'Washington Commanders',
    'WSH':'Washington Commanders'
}
FULL_TO_CODE = {v:k for k,v in TEAM_CODE_TO_FULL.items()}

# Stadium coordinates (lat,lon) for Open-Meteo
STADIUM_COORDS = {
    'ARI': (33.5275, -112.2626), 'ATL': (33.7554, -84.4008), 'BAL': (39.2780, -76.6227),
    'BUF': (42.7738, -78.7870), 'CAR': (35.2258, -80.8528), 'CHI': (41.8623, -87.6167),
    'CIN': (39.0954, -84.5161), 'CLE': (41.5061, -81.6995), 'DAL': (32.7473, -97.0945),
    'DEN': (39.7439, -105.0201), 'DET': (42.3400, -83.0456), 'GB': (44.5013, -88.0622),
    'HOU': (29.6847, -95.4107), 'IND': (39.7601, -86.1639), 'JAX': (30.3239, -81.6374),
    'KC': (39.0490, -94.4839), 'LAC': (33.9535, -118.3387), 'LAR': (33.9535, -118.3387),
    'LV': (36.0909, -115.1830), 'MIA': (25.9580, -80.2389), 'MIN': (44.9738, -93.2577),
    'NE': (42.0909, -71.2643), 'NO': (29.9509, -90.0815), 'NYG': (40.8128, -74.0742),
    'NYJ': (40.8128, -74.0742), 'PHI': (39.9008, -75.1675), 'PIT': (40.4468, -80.0158),
    'SEA': (47.5952, -122.3316), 'SF': (37.4030, -121.9700), 'TB': (27.9759, -82.5033),
    'TEN': (36.1665, -86.7713), 'WAS': (38.9078, -76.8645), 'WSH': (38.9078, -76.8645),
}

OUTPUT_BASE = "/content/drive/MyDrive/NFL_Bot"
DATA_OUT = os.path.join(OUTPUT_BASE, "data")
os.makedirs(DATA_OUT, exist_ok=True)


# =========================
# 2) Robust nfl_data_py loader (skips missing season parquets)
# =========================
def _concat_per_year_safely(import_fn, years, label):
    frames, used = [], []
    for y in years:
        try:
            df = import_fn([y])  # nfl_data_py expects list of years
            if df is not None and len(df):
                frames.append(df); used.append(y)
        except Exception as e:
            print(f"{label}: skipping {y} -> {type(e).__name__}: {e}")
    if frames:
        out = pd.concat(frames, ignore_index=True)
        print(f"{label}: loaded years {used}")
        return out
    print(f"{label}: no data loaded for {years}")
    return pd.DataFrame()

def load_core_data(years: List[int]) -> Dict[str, pd.DataFrame]:
    print(f"Loading core datasets for years: {years}")
    data: Dict[str, pd.DataFrame] = {}
    data["weekly"]       = _concat_per_year_safely(nfl.import_weekly_data,   years, "weekly")
    data["injuries"]     = _concat_per_year_safely(nfl.import_injuries,      years, "injuries")
    data["depth_charts"] = _concat_per_year_safely(nfl.import_depth_charts,  years, "depth_charts")
    data["schedules"]    = _concat_per_year_safely(nfl.import_schedules,     years, "schedules")
    data["sc_lines"]     = _concat_per_year_safely(nfl.import_sc_lines,      years, "sc_lines")
    for stat in ("receiving","rushing","passing"):
        try:
            fn = lambda ys: nfl.import_ngs_data(stat, ys)
            data[f"ngs_{stat}"] = _concat_per_year_safely(fn, years, f"ngs_{stat}")
        except Exception as e:
            print(f"ngs_{stat}: error -> {e}")
            data[f"ngs_{stat}"] = pd.DataFrame()
    return data


# =========================
# 3) The Odds API helpers
# =========================
def call_the_odds_api_odds(
    sport: str = 'americanfootball_nfl',
    regions: str = 'us',
    markets: str = 'h2h,spreads,totals',
    bookmakers: Optional[str] = None,  # e.g., 'fanduel'
    commence_from_iso: Optional[str] = None,
    commence_to_iso: Optional[str] = None,
    timeout: int = 30
) -> Tuple[list, dict]:
    key = os.getenv('ODDS_API_KEY')
    if not key:
        raise RuntimeError('ODDS_API_KEY missing.')

    url = f'https://api.the-odds-api.com/v4/sports/{sport}/odds'
    params = {
        'apiKey': key,
        'regions': regions,
        'markets': markets,
        'oddsFormat': 'american',
    }
    if commence_from_iso: params['commenceTimeFrom'] = commence_from_iso
    if commence_to_iso:   params['commenceTimeTo']   = commence_to_iso
    if bookmakers:        params['bookmakers']       = bookmakers

    r = requests.get(url, params=params, timeout=timeout)
    r.raise_for_status()
    headers = {'x-requests-remaining': r.headers.get('x-requests-remaining'),
               'x-requests-used': r.headers.get('x-requests-used')}
    return r.json(), headers

def flatten_odds_events(events: list) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if not events:
        return pd.DataFrame(), pd.DataFrame()
    events_df = pd.json_normalize(events)
    keep = ['id','commence_time','home_team','away_team','sport_key']
    for c in keep:
        if c not in events_df.columns: events_df[c] = None
    events_df = events_df[keep].rename(columns={'id':'event_id'})

    rows = []
    for e in events:
        eid = e.get('id'); commence = e.get('commence_time')
        home = e.get('home_team'); away = e.get('away_team')
        for bk in e.get('bookmakers', []) or []:
            bkey = bk.get('key'); btitle = bk.get('title'); updated = bk.get('last_update')
            for mkt in bk.get('markets', []) or []:
                mkey = mkt.get('key')
                for oc in mkt.get('outcomes', []) or []:
                    rows.append({
                        'event_id': eid, 'commence_time': commence, 'home_team': home, 'away_team': away,
                        'bookmaker_key': bkey, 'bookmaker': btitle, 'last_update': updated,
                        'market': mkey, 'name': oc.get('name'), 'price': oc.get('price'), 'point': oc.get('point')
                    })
    odds_df = pd.DataFrame(rows)
    return events_df, odds_df

def map_events_to_codes(events_df: pd.DataFrame) -> pd.DataFrame:
    out = events_df.copy()
    out['home_code'] = out['home_team'].map(FULL_TO_CODE)
    out['away_code'] = out['away_team'].map(FULL_TO_CODE)
    return out


# =========================
# 4) Open-Meteo weather (free, no key)
# =========================
def om_forecast_near_kickoff(lat: float, lon: float, kickoff_ts: pd.Timestamp,
                             variables: str = 'temperature_2m,wind_speed_10m,precipitation_probability',
                             timeout: int = 30) -> Optional[dict]:
    base = 'https://api.open-meteo.com/v1/forecast'
    params = {'latitude': lat, 'longitude': lon, 'hourly': variables,
              'forecast_days': 16, 'temperature_unit': 'fahrenheit',
              'wind_speed_unit': 'mph', 'timezone': 'auto'}
    r = requests.get(base, params=params, timeout=timeout)
    if r.status_code != 200:
        print('Open-Meteo error:', r.status_code, r.text[:160]); return None
    js = r.json() or {}
    hourly = js.get('hourly') or {}
    times = hourly.get('time') or []
    if not times: return None
    target = pd.to_datetime(kickoff_ts)
    tseries = pd.to_datetime(pd.Series(times))
    idx = int((tseries - target).abs().idxmin())
    def _get(field, default=None):
        arr = hourly.get(field) or []
        return arr[idx] if idx < len(arr) else default
    return {
        'lat': lat, 'lon': lon,
        'time_near_kickoff': times[idx] if idx < len(times) else None,
        'temp_f': _get('temperature_2m'),
        'wind_mph': _get('wind_speed_10m'),
        'precip_prob_pct': _get('precipitation_probability'),
        'raw_hourly_sample': {k: (v[idx] if isinstance(v, list) and idx < len(v) else None)
                              for k, v in hourly.items() if isinstance(v, list)}
    }

def open_meteo_for_game(home_code: str, kickoff_ts: Optional[pd.Timestamp]) -> Optional[dict]:
    if home_code not in STADIUM_COORDS or kickoff_ts is None or pd.isna(kickoff_ts): return None
    lat, lon = STADIUM_COORDS[home_code]
    return om_forecast_near_kickoff(lat, lon, kickoff_ts)


# =========================
# 5) Injury snapshot (schema-agnostic)
# =========================
def latest_injury_snapshot(injuries_df: pd.DataFrame, season: int) -> pd.DataFrame:
    if injuries_df is None or injuries_df.empty: return pd.DataFrame()
    df = injuries_df.copy()
    if 'season' in df.columns: df = df[df['season']==season].copy()
    if 'player_name' not in df.columns:
        src = next((c for c in ['full_name','player','display_name','gsis_name'] if c in df.columns), None)
        if src: df['player_name'] = df[src].astype(str)
        elif {'first_name','last_name'}.issubset(df.columns):
            df['player_name'] = (df['first_name'].fillna('') + ' ' + df['last_name'].fillna('')).str.strip()
        else:
            df['player_name'] = df.get('player_id', pd.Series(['']*len(df), index=df.index)).astype(str)
    team_src = next((c for c in ['team','team_abbr','club','club_code','team_code'] if c in df.columns), None)
    if team_src and team_src!='team': df = df.rename(columns={team_src:'team'})
    if 'week' not in df.columns:
        wk_src = next((c for c in ['game_week','week_number'] if c in df.columns), None)
        df = df.rename(columns={wk_src:'week'}) if wk_src else df.assign(week=pd.NA)
    keep = ['season','week','team','player_id','player_name','position','practice','practice_status',
            'report_status','game_status','body_part','injury_notes','status_date','report_date','updated']
    keep = [c for c in keep if c in df.columns]
    out = df[keep].copy() if keep else df.copy()
    sort_cols = [c for c in ['week','team','player_name'] if c in out.columns]
    if sort_cols: out = out.sort_values(sort_cols, na_position='last')
    return out.reset_index(drop=True)


# =========================
# 6) Kickoff parser (ET combine)
# =========================
def _parse_kickoff(row: pd.Series) -> Optional[pd.Timestamp]:
    for c in ['kickoff','datetime','gamedatetime','game_start','start_time','start_time_et','game_time']:
        if c in row and pd.notna(row[c]):
            ts = pd.to_datetime(row[c], utc=False, errors='coerce')
            if pd.notna(ts): return ts
    gd, gt = row.get('gameday'), row.get('gametime')
    if pd.notna(gd) and pd.notna(gt):
        try:
            return pd.Timestamp(f"{gd} {gt}").tz_localize(ZoneInfo("America/New_York")).tz_convert(None)
        except Exception:
            ts = pd.to_datetime(f"{gd} {gt}", errors='coerce')
            if pd.notna(ts): return ts
    if pd.notna(row.get('gameday')):   return pd.to_datetime(row['gameday'], errors='coerce')
    if pd.notna(row.get('game_date')): return pd.to_datetime(row['game_date'], errors='coerce')
    return None


# =========================
# 7) Weekly bundle
# =========================
def get_week_bundle(season: int, week: int, years_back: int = 3, fd_only_fetch: bool=False) -> Dict[str, Any]:
    years = list(range(max(2019, season - years_back), season + 1))
    core = load_core_data(years)
    sched = core['schedules']
    week_games = sched[(sched['season']==season) & (sched['week']==week)].copy()
    week_games['kickoff'] = week_games.apply(_parse_kickoff, axis=1)

    # FanDuel-only fetch toggle
    bookmakers = 'fanduel' if fd_only_fetch else None
    events_json, headers = call_the_odds_api_odds(bookmakers=bookmakers)
    events_df, odds_df = flatten_odds_events(events_json)
    events_df = map_events_to_codes(events_df)

    weather_rows = []
    for _, g in week_games.iterrows():
        home = g.get('home_team'); kdt = g.get('kickoff')
        wx = open_meteo_for_game(home, kdt) if isinstance(home, str) else None
        weather_rows.append({'season': season, 'week': week, 'home_team': home, 'kickoff': kdt, 'weather': wx})

    bundle = {
        'schedule': week_games.reset_index(drop=True),
        'scoring_lines': core.get('sc_lines'),
        'injuries': latest_injury_snapshot(core['injuries'], season),
        'odds_events': events_df,
        'odds_long': odds_df,
        'odds_headers': headers,
        'weather_by_game': pd.DataFrame(weather_rows)
    }
    return bundle


# =========================
# 8) FanDuel-first implied totals
# =========================
PREFERRED_BOOKS = ('fanduel','draftkings','betmgm','caesars','pointsbetus','bovada')

def pick_bookmaker(odds_long: pd.DataFrame, preferred=PREFERRED_BOOKS):
    df = odds_long.copy()
    df['bk_rank'] = df['bookmaker_key'].map({b:i for i,b in enumerate(preferred)}).fillna(9999)
    df = df.sort_values(['event_id','market','name','bk_rank','last_update'])
    return df.groupby(['event_id','market','name'], as_index=False).first()

def build_implied_totals(bundle: Dict[str, Any], strict_fanduel: bool=False) -> pd.DataFrame:
    ol = bundle['odds_long'].copy()
    if strict_fanduel:
        ol = ol[ol['bookmaker_key'].str.lower()=='fanduel']
    od1 = pick_bookmaker(ol)

    totals = od1[(od1['market']=='totals') & (od1['name'].str.lower()=='over')][['event_id','point','bookmaker_key']]
    totals = totals.rename(columns={'point':'total_line','bookmaker_key':'totals_book'})

    ev = bundle['odds_events'][['event_id','home_team','away_team','home_code','away_code']].drop_duplicates()
    spreads = od1[od1['market']=='spreads'][['event_id','name','point','bookmaker_key']]
    hs = (spreads.merge(ev[['event_id','home_team']], on='event_id', how='left')
                .query('name == home_team')[['event_id','point','bookmaker_key']]
                .rename(columns={'point':'home_spread','bookmaker_key':'spreads_book'}))

    ev2 = ev.merge(totals, on='event_id', how='left').merge(hs, on='event_id', how='left')
    ev2['home_tt'] = ev2.apply(lambda r: (r['total_line']/2) - (r['home_spread']/2)
                               if pd.notna(r['total_line']) and pd.notna(r['home_spread']) else None, axis=1)
    ev2['away_tt'] = ev2.apply(lambda r: (r['total_line'] - r['home_tt']) if pd.notna(r.get('home_tt')) else None, axis=1)

    # Join schedule; expose both codes & full names cleanly
    wk = bundle['schedule'][['game_id','season','week','home_team','away_team','kickoff']].copy()
    wk = wk.rename(columns={'home_team':'home_code','away_team':'away_code'})
    wk['home_full'] = wk['home_code'].map(TEAM_CODE_TO_FULL)
    wk['away_full'] = wk['away_code'].map(TEAM_CODE_TO_FULL)

    j = (wk.merge(ev2, left_on=['home_full','away_full'], right_on=['home_team','away_team'], how='left')
           .drop(columns=['home_team','away_team']))

    cols = ['game_id','season','week','kickoff',
            'home_code','away_code','home_full','away_full',
            'total_line','home_spread','home_tt','away_tt','totals_book','spreads_book']
    cols = [c for c in cols if c in j.columns]
    return j[cols]


# =========================
# 9) Weather flattener & contexts
# =========================
def expand_weather(bundle: Dict[str, Any]) -> pd.DataFrame:
    w = bundle['weather_by_game'].copy()
    for k in ['temp_f','wind_mph','precip_prob_pct','time_near_kickoff','lat','lon']:
        w[f'wx_{k}'] = w['weather'].apply(lambda d: d.get(k) if isinstance(d, dict) else None)
    return w

def attach_contexts(bundle, data_root="/content/drive/MyDrive/NFL_Bot/data"):
    coach_pq = os.path.join(data_root, "coach_context.parquet")
    ref_pq   = os.path.join(data_root, "ref_context.parquet")
    wk = bundle['schedule'][['game_id']].drop_duplicates()

    coach = pd.read_parquet(coach_pq) if os.path.exists(coach_pq) else pd.DataFrame()
    ref   = pd.read_parquet(ref_pq)   if os.path.exists(ref_pq)   else pd.DataFrame()

    coach_cut = coach[coach['game_id'].isin(wk['game_id'])].copy() if not coach.empty else pd.DataFrame()
    ref_cut   = ref[ref['game_id'].isin(wk['game_id'])].copy()     if not ref.empty   else pd.DataFrame()

    bundle['coach_ctx'] = coach_cut.reset_index(drop=True)
    bundle['ref_ctx']   = ref_cut.reset_index(drop=True)
    return bundle


# =========================
# 10) Utilities: auto_week + small data-access helpers
# =========================
def auto_week(season: int) -> int:
    core = load_core_data([season])
    s = core.get('schedules', pd.DataFrame()).copy()
    if s.empty: return 1
    def _kick(row):
        for c in ['kickoff','datetime','gamedatetime','game_start','start_time','start_time_et','gameday','game_date']:
            if c in s.columns and pd.notna(row.get(c)):
                ts = pd.to_datetime(row[c], errors='coerce')
                if pd.notna(ts): return ts
        return pd.NaT
    s['kickoff_ts'] = s.apply(_kick, axis=1)
    future = s[(s['season']==season) & s['kickoff_ts'].notna() & (s['kickoff_ts'] >= pd.Timestamp.now())]
    if not future.empty: return int(future.sort_values('kickoff_ts').iloc[0]['week'])
    wks = s.loc[s['season']==season, 'week'].dropna().astype(int)
    return int(wks.max()) if not wks.empty else 1

def get_coach_for_team(bundle, team_code):
    df = bundle.get('coach_ctx', pd.DataFrame())
    if df.empty: return {}
    r = df[df['team']==team_code].dropna(axis=1, how='all').head(1)
    return r.to_dict(orient='records')[0] if not r.empty else {}

def get_ref_for_game(bundle, game_id):
    df = bundle.get('ref_ctx', pd.DataFrame())
    if df.empty: return {}
    r = df[df['game_id']==game_id].dropna(axis=1, how='all').head(1)
    return r.to_dict(orient='records')[0] if not r.empty else {}


# =========================
# 11) Coach & Ref Tendencies Builders (writes parquet to /NFL_Bot/data)
# =========================
def build_coach_ref_contexts(years_back: int = 4):
    _current_year = dt.datetime.now().year
    YEARS_TEND = list(range(max(2019, _current_year-years_back), _current_year+1))

    pbp = _concat_per_year_safely(nfl.import_pbp_data, YEARS_TEND, "pbp")
    sched = _concat_per_year_safely(nfl.import_schedules, YEARS_TEND, "schedules")
    try:
        officials = _concat_per_year_safely(nfl.import_officials, YEARS_TEND, "officials")
    except Exception as e:
        print("officials: not available ->", e); officials = pd.DataFrame(columns=["game_id"])

    # ---------- PBP prep ----------
    def prep_pbp(df: pd.DataFrame) -> pd.DataFrame:
        d = df.copy()
        if "pass_attempt" not in d.columns:
            d["pass_attempt"] = (d.get("play_type","").astype(str).str.lower()=="pass").astype("Int64")
        if "rush_attempt" not in d.columns:
            d["rush_attempt"] = (d.get("play_type","").astype(str).str.lower()=="run").astype("Int64")
        for c in ["season","week","posteam","qtr","down","yardline_100","ydstogo","score_differential","game_id"]:
            if c not in d.columns: d[c] = pd.NA
        d["neutral"] = d["qtr"].between(1,3) & (d["score_differential"].abs()<=7 if "score_differential" in d else True)
        d["is_rz"] = d["yardline_100"].le(20)
        d["is_gl"] = d["yardline_100"].le(5)
        if "game_seconds_remaining" in d.columns:
            d = d.sort_values(["game_id","posteam","qtr","down"])
            d["sec_elapsed"] = d.groupby(["game_id","posteam"])["game_seconds_remaining"].diff(-1).abs()
        else:
            d["sec_elapsed"] = pd.NA
        if "personnel_offense" not in d.columns and "offense_personnel" in d.columns:
            d["personnel_offense"] = d["offense_personnel"]
        return d

    pbp = prep_pbp(pbp)

    # ---------- team-week agg ----------
    def _personnel_tag(s: str):
        if not isinstance(s,str): return None
        m_rb = re.search(r'(\d+)\s*RB', s, re.I)
        m_te = re.search(r'(\d+)\s*TE', s, re.I)
        if not m_rb or not m_te: return None
        return f"{int(m_rb.group(1))}{int(m_te.group(1))}"

    def _rate(p, q):
        num = p.fillna(0).sum(); den = (p.fillna(0)+q.fillna(0)).sum()
        return float(num/den) if den>0 else np.nan

    def agg_team_week(g: pd.DataFrame) -> pd.Series:
        g_neu = g[g["neutral"].fillna(False)]
        g_fd  = g_neu[g_neu["down"].eq(1)]
        g_ed  = g_neu[g_neu["down"].isin([1,2])]
        first_down_pr = _rate(g_fd["pass_attempt"], g_fd["rush_attempt"])
        early_down_pr = _rate(g_ed["pass_attempt"], g_ed["rush_attempt"])
        pace = g_neu["sec_elapsed"].dropna().mean() if "sec_elapsed" in g_neu and not g_neu["sec_elapsed"].dropna().empty else np.nan
        rz = g[g["is_rz"].fillna(False)]
        rz_plays=(rz["pass_attempt"].fillna(0)+rz["rush_attempt"].fillna(0)).sum()
        rz_run=float(rz["rush_attempt"].fillna(0).sum()/rz_plays) if rz_plays>0 else np.nan
        gl = g[g["is_gl"].fillna(False)]
        gl_plays=(gl["pass_attempt"].fillna(0)+gl["rush_attempt"].fillna(0)).sum()
        gl_run=float(gl["rush_attempt"].fillna(0).sum()/gl_plays) if gl_plays>0 else np.nan
        p11=p12=p21=np.nan
        if "personnel_offense" in g.columns:
            tags=g["personnel_offense"].apply(_personnel_tag)
            if len(tags):
                p11=(tags=="11").mean(); p12=(tags=="12").mean(); p21=(tags=="21").mean()
        fourth=np.nan
        if all(c in g.columns for c in ["down","ydstogo","yardline_100"]):
            mid=g[(g["down"]==4)&(g["ydstogo"].le(2))&g["yardline_100"].between(40,60)]
            att=len(mid); went=((mid["pass_attempt"].fillna(0)+mid["rush_attempt"].fillna(0))>0).sum()
            fourth=float(went/att) if att>0 else np.nan
        return pd.Series({
            "neutral_first_down_pass_rate": first_down_pr,
            "neutral_early_down_pass_rate": early_down_pr,
            "neutral_pace_sec_per_play":    pace,
            "rz_run_rate":                  rz_run,
            "goal_to_go_run_rate":          gl_run,
            "personnel_11_share":           p11,
            "personnel_12_share":           p12,
            "personnel_21_share":           p21,
            "fourth_down_aggr_index":       fourth,
        })

    tw = (pbp.groupby(["season","week","posteam"], as_index=False)
             .apply(agg_team_week, include_groups=True)  # keep grouping cols
             .reset_index(drop=True)
             .rename(columns={"posteam":"team"}))

    def roll_last4(g: pd.DataFrame) -> pd.DataFrame:
        g = g.sort_values(["season","week"])
        cols = ["neutral_first_down_pass_rate","neutral_early_down_pass_rate",
                "neutral_pace_sec_per_play","rz_run_rate","goal_to_go_run_rate",
                "personnel_11_share","personnel_12_share","personnel_21_share",
                "fourth_down_aggr_index"]
        avail = [c for c in cols if c in g.columns]
        rolled = g[avail].shift(1).rolling(window=4, min_periods=1).mean() if avail else pd.DataFrame(index=g.index)
        if not rolled.empty:
            rolled.columns = [f"{c}_last4" for c in avail]
            out = pd.concat([g[["season","week","team"]], rolled], axis=1)
        else:
            out = g[["season","week","team"]].copy()
        return out

    tw_roll = (tw.groupby("team", group_keys=False)
                 .apply(roll_last4, include_groups=True)
                 .reset_index(drop=True))

    # ---------- Coach mapping ----------
    home_coach_col = next((c for c in sched.columns if c.lower().startswith("home_coach")), None)
    away_coach_col = next((c for c in sched.columns if c.lower().startswith("away_coach")), None)

    coach_rows = []
    for _, r in sched.iterrows():
        s, w = r.get("season"), r.get("week")
        if pd.isna(s) or pd.isna(w): continue
        coach_rows.append({"game_id": r.get("game_id"), "season": int(s), "week": int(w),
                           "team": r.get("home_team"), "is_home": True,
                           "coach_name": r.get(home_coach_col) if home_coach_col else None})
        coach_rows.append({"game_id": r.get("game_id"), "season": int(s), "week": int(w),
                           "team": r.get("away_team"), "is_home": False,
                           "coach_name": r.get(away_coach_col) if away_coach_col else None})
    coach_map = pd.DataFrame(coach_rows)

    def coach_id_hash(name, season, team):
        key = (str(name) if pd.notna(name) and str(name).strip() else f"{season}_{team}_coach").encode()
        return int(hashlib.sha1(key).hexdigest()[:8], 16)

    if not coach_map.empty:
        coach_map["coach_id"] = coach_map.apply(lambda r: coach_id_hash(r["coach_name"], r["season"], r["team"]), axis=1)
        coach_ctx = coach_map.merge(tw_roll, on=["season","week","team"], how="left")

        # carry-forward last available rolling values for empty week-1 rows
        last4_cols = [c for c in coach_ctx.columns if c.endswith("_last4")]
        if last4_cols:
            latest = (tw_roll.sort_values(["team","season","week"])
                            .groupby("team").tail(1)[["team"]+last4_cols].set_index("team"))
            for c in last4_cols:
                fillmap = latest[c].to_dict()
                needs = coach_ctx[c].isna()
                coach_ctx.loc[needs, c] = coach_ctx.loc[needs, "team"].map(fillmap)
    else:
        coach_ctx = pd.DataFrame(columns=["game_id","season","week","team","is_home","coach_name","coach_id"])

    coach_out = os.path.join(DATA_OUT, "coach_context.parquet")
    coach_ctx.to_parquet(coach_out, index=False)
    print("[write]", coach_out, "rows=", len(coach_ctx))

    # ---------- Ref mapping ----------
    def build_ref_map(officials_df: pd.DataFrame) -> pd.DataFrame:
        if officials_df is None or officials_df.empty:
            return pd.DataFrame(columns=["game_id","ref_name"])
        cols_lc = {c.lower(): c for c in officials_df.columns}
        pos_col = next((cols_lc[k] for k in cols_lc if re.search(r"(position|assignment|role)", k)), None)
        name_col = next((cols_lc[k] for k in cols_lc if re.search(r"(full.*name|official.*name|name)$", k)), None)
        first_col = next((cols_lc[k] for k in cols_lc if re.search(r"(first.*name)", k)), None)
        last_col  = next((cols_lc[k] for k in cols_lc if re.search(r"(last.*name)",  k)), None)
        if pos_col and (name_col or (first_col and last_col)):
            tmp = officials_df[["game_id", pos_col] + ([name_col] if name_col else [first_col, last_col])].copy()
            tmp[pos_col] = tmp[pos_col].astype(str).str.lower()
            tmp = tmp[tmp[pos_col].str.contains(r"\bref(?:eree)?\b")]
            if name_col: tmp["ref_name"] = tmp[name_col].astype(str).str.strip()
            else: tmp["ref_name"] = (tmp[first_col].astype(str).str.strip()+" "+tmp[last_col].astype(str).str.strip()).str.strip()
            return tmp.sort_values(["ref_name"]).drop_duplicates("game_id")[["game_id","ref_name"]]
        # wide/crew fallback
        def melt_wide(officials_df) -> pd.DataFrame:
            have = []
            for k, v in cols_lc.items():
                if any(t in k for t in ["referee","ref","umpire","judge","official","crew"]):
                    have.append(v)
            if not have:
                return pd.DataFrame(columns=["game_id","role_col","ref_name"])
            wide = officials_df[["game_id"] + list(dict.fromkeys(have))].copy()
            long = wide.melt(id_vars="game_id", var_name="role_col", value_name="ref_name")
            long["role_col_lc"] = long["role_col"].str.lower()
            long["ref_name"] = long["ref_name"].astype(str).str.strip()
            long = long[long["ref_name"] != ""]
            return long
        long = melt_wide(officials)
        if not long.empty:
            ref_rows = long[ long["role_col_lc"].str.contains(r"\breferee\b") ]
            if ref_rows.empty: ref_rows = long[ long["role_col_lc"].str.contains(r"\bref\b") ]
            if not ref_rows.empty:
                return ref_rows.sort_values(["ref_name"]).drop_duplicates("game_id")[["game_id","ref_name"]]
        crew_col = next((c for c in cols_lc if "crew" in c or "official" in c), None)
        if crew_col:
            tmp = officials[["game_id", cols_lc[crew_col]]].copy()
            tmp["ref_name"] = tmp[cols_lc[crew_col]].astype(str).str.extract(r"Referee:\s*([A-Za-z .'-]+)", expand=False).str.strip()
            tmp = tmp.dropna(subset=["ref_name"])
            if not tmp.empty:
                return tmp.drop_duplicates("game_id")[["game_id","ref_name"]]
        return pd.DataFrame(columns=["game_id","ref_name"])

    ref_map = build_ref_map(officials)

    # ensure every scheduled game has a row (fallback neutral ref if missing)
    all_games = sched[["game_id","season","week"]].drop_duplicates()
    if ref_map.empty:
        ref_map = all_games.copy(); ref_map["ref_name"] = "Neutral_Ref"
    else:
        missing = all_games[~all_games["game_id"].isin(ref_map["game_id"])]
        if not missing.empty:
            add = missing.copy(); add["ref_name"] = "Neutral_Ref"
            ref_map = pd.concat([ref_map, add[["game_id","ref_name"]]], ignore_index=True)

    ref_map["ref_id"] = ref_map["ref_name"].map(lambda s: int(hashlib.sha1(str(s).encode()).hexdigest()[:8], 16))

    # ---------- penalty profile ----------
    pp = pbp.copy()
    if "pass_attempt" not in pp.columns:
        pp["pass_attempt"] = (pp.get("play_type","").astype(str).str.lower()=="pass").astype("Int64")
    if "rush_attempt" not in pp.columns:
        pp["rush_attempt"] = (pp.get("play_type","").astype(str).str.lower()=="run").astype("Int64")

    desc_col = next((c for c in pp.columns if "desc" in c.lower() or "play_description" in c.lower()), None)
    if "penalty" not in pp.columns:
        if desc_col: pp["penalty"] = pp[desc_col].astype(str).str.contains("penalty", case=False, na=False).astype("Int64")
        else: pp["penalty"] = pd.NA

    if desc_col:
        dlow = pp[desc_col].astype(str).str.lower()
        dpi_mask  = dlow.str.contains(r"defensive pass interference| dpi\b", regex=True)
        hold_mask = dlow.str.contains(r"offensive holding|holding on the offense|holding \(off", regex=True)
    else:
        dpi_mask  = pd.Series(False, index=pp.index)
        hold_mask = pd.Series(False, index=pp.index)

    pp["is_play"] = ((pp["pass_attempt"].fillna(0)+pp["rush_attempt"].fillna(0))>0).astype(int)

    grp = pp.groupby("game_id", as_index=False).agg(
        plays=("is_play","sum"),
        pass_att=("pass_attempt","sum"),
        penalties=("penalty","sum"),
        dpi=("is_play", lambda s: int(dpi_mask.loc[s.index].sum())),
        off_hold=("is_play", lambda s: int(hold_mask.loc[s.index].sum())),
    )
    grp["penalties_per100"]      = np.where(grp["plays"]>0, grp["penalties"]*100.0/grp["plays"], np.nan)
    grp["dpi_per100_pass"]       = np.where(grp["pass_att"]>0, grp["dpi"]*100.0/grp["pass_att"], np.nan)
    grp["off_hold_per100_plays"] = np.where(grp["plays"]>0, grp["off_hold"]*100.0/grp["plays"], np.nan)

    # ---------- kickoff + season/week join ----------
    # kickoff_ts
    kick_col = next((c for c in ['kickoff','datetime','gamedatetime','game_start',
                                 'start_time','start_time_et','gameday','game_date'] if c in sched.columns), None)
    sched_dt = sched[['game_id']].copy()
    if kick_col:
        sched_dt = sched[['game_id', kick_col]].copy()
        sched_dt['kickoff_ts'] = pd.to_datetime(sched_dt[kick_col], utc=False, errors='coerce')
        sched_dt = sched_dt[['game_id','kickoff_ts']]
    else:
        sched_dt['kickoff_ts'] = pd.NaT

    # guarantee 'season','week' keys exist for merge
    sched_keys = sched[['game_id','season','week']].drop_duplicates()

    ref_games = (grp.merge(ref_map[['game_id','ref_name','ref_id']], on="game_id", how="right")
                   .merge(sched_dt, on="game_id", how="left")
                   .merge(sched_keys, on="game_id", how="left")
                   .sort_values(['ref_id','kickoff_ts'], na_position='last'))

    # --- SAFE rolling last-16 per ref (index-aligned) ---
    metric_cols = ["penalties_per100","dpi_per100_pass","off_hold_per100_plays"]
    for col in metric_cols:
        s = (ref_games.groupby('ref_id', dropna=False)[col]
                      .apply(lambda s_: s_.shift(1).rolling(window=16, min_periods=1).mean()))
        s = s.reset_index(level=0, drop=True)  # align index back to ref_games
        ref_games[f"{col}_last16"] = s

    # final ref context view (only select cols that exist to avoid KeyError)
    ref_cols = ["game_id","ref_id","ref_name","season","week","kickoff_ts",
                "penalties_per100_last16","dpi_per100_pass_last16","off_hold_per100_plays_last16"]
    ref_cols = [c for c in ref_cols if c in ref_games.columns]
    ref_ctx = ref_games[ref_cols].copy()

    ref_out = os.path.join(DATA_OUT, "ref_context.parquet")
    ref_ctx.to_parquet(ref_out, index=False)
    print("[write]", ref_out, "rows=", len(ref_ctx))

    return {"coach_ctx": coach_ctx, "ref_ctx": ref_ctx,
            "coach_path": coach_out, "ref_path": ref_out}


# =========================
# 12) Example: run everything
# =========================
from datetime import datetime, timezone
today = datetime.now(timezone.utc).date()
example_season = today.year
example_week = 1  # or: auto_week(example_season)

print(f"Fetching bundle for season={example_season}, week={example_week} ...")
bundle = get_week_bundle(example_season, example_week, fd_only_fetch=False)  # set True for FD-only fetch

# build contexts (once per session, or when you want refreshed rolling windows)
_ = build_coach_ref_contexts(years_back=4)

# attach to bundle
bundle = attach_contexts(bundle)

print("\nSchedule (head):")
display(bundle['schedule'].head(3))

print("\nOdds events (mapped to codes), head:")
display(bundle['odds_events'].head(3))

print("\nOdds long (first 5 rows):")
display(bundle['odds_long'].head(5))

print("\nWeather by game (first 3 rows):")
wx = expand_weather(bundle)
display(wx[['season','week','home_team','kickoff','wx_time_near_kickoff','wx_temp_f','wx_wind_mph','wx_precip_prob_pct']].head(3))

print("\nInjuries snapshot (head):")
display(bundle['injuries'].head(5))

print("\nFanDuel-first implied totals (head):")
imp = build_implied_totals(bundle, strict_fanduel=False)
display(imp.head(10))

print("\nCoach context (head):")
display(bundle['coach_ctx'].head(6))

print("\nRef context (head):")
display(bundle['ref_ctx'].head(6))

print("\nAPI usage headers:", bundle['odds_headers'])

# =========================
# 13) LLM PICKS (strict JSON with robust fallbacks)
# =========================
from openai import OpenAI
import json, re
client = OpenAI()  # single OpenAI client for the notebook

def build_game_packets(bundle: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Merge implied totals, weather, coach & ref context into small JSON packets per game.
    Robust to different columns returned by build_implied_totals (with/without home_code/away_code).
    """
    # 1) Data sources
    imp   = build_implied_totals(bundle, strict_fanduel=False)  # may or may not include home_code/away_code
    wx    = expand_weather(bundle)[["home_team","kickoff","wx_temp_f","wx_wind_mph","wx_precip_prob_pct"]] \
              .rename(columns={"home_team":"home_code"})
    coach = bundle.get("coach_ctx", pd.DataFrame())
    refc  = bundle.get("ref_ctx", pd.DataFrame())
    sched = bundle["schedule"][["game_id","season","week","home_team","away_team","kickoff"]] \
              .rename(columns={"home_team":"home_code","away_team":"away_code"})

    # 2) Coach context → home/away wide (last4 metrics only)
    if not coach.empty:
        last4 = [c for c in coach.columns if c.endswith("_last4")]
        h = coach[coach["is_home"]==True ][["game_id","team","coach_name"]+last4] \
               .rename(columns={"team":"home_code","coach_name":"home_coach"})
        a = coach[coach["is_home"]==False][["game_id","team","coach_name"]+last4] \
               .rename(columns={"team":"away_code","coach_name":"away_coach"})
        for c in last4:
            h.rename(columns={c:f"{c}_home"}, inplace=True)
            a.rename(columns={c:f"{c}_away"}, inplace=True)
        coach_wide = h.merge(a, on="game_id", how="outer")
    else:
        coach_wide = pd.DataFrame()

    # 3) Ref context (compact)
    if not refc.empty:
        ref_keep = ["game_id","ref_name","penalties_per100_last16","dpi_per100_pass_last16","off_hold_per100_plays_last16"]
        ref_small = refc[[c for c in ref_keep if c in refc.columns]].drop_duplicates("game_id")
    else:
        ref_small = pd.DataFrame(columns=["game_id","ref_name"])

    # 4) Merge schedule with implied totals — auto-detect common keys
    preferred_keys = ["game_id","season","week","kickoff","home_code","away_code"]
    common_keys = [k for k in preferred_keys if k in sched.columns and k in imp.columns] or ["game_id"]
    base = sched.merge(imp, on=common_keys, how="left")

    # 5) Merge weather (needs home_code + kickoff). If missing, try looser merge.
    if {"home_code","kickoff"}.issubset(base.columns):
        base = base.merge(wx, on=["home_code","kickoff"], how="left")
    elif "home_code" in base.columns:
        base = base.merge(wx.drop(columns=["kickoff"]), on=["home_code"], how="left")

    # 6) Coach/Ref
    if not coach_wide.empty:
        base = base.merge(coach_wide, on="game_id", how="left")
    if not ref_small.empty:
        base = base.merge(ref_small, on="game_id", how="left")

    # 7) Build packets
    def _safe(x, nd=3):
        try:
            if x is None or (isinstance(x, float) and (pd.isna(x) or np.isinf(x))):
                return None
            return round(float(x), nd)
        except Exception:
            return None

    packets = []
    for _, r in base.iterrows():
        home_code = r.get("home_code")
        away_code = r.get("away_code")
        packets.append({
            "game_id": r.get("game_id"),
            "season": int(r["season"]) if pd.notna(r.get("season")) else None,
            "week": int(r["week"]) if pd.notna(r.get("week")) else None,
            "kickoff": str(r.get("kickoff")) if pd.notna(r.get("kickoff")) else None,
            "home": {
                "code": home_code,
                "name": TEAM_CODE_TO_FULL.get(home_code, home_code),
                "implied_total": _safe(r.get("home_tt")),
                "coach": r.get("home_coach"),
                "tendencies_last4": {
                    "early_down_pr": _safe(r.get("neutral_early_down_pass_rate_last4_home")),
                    "pace_sec_play": _safe(r.get("neutral_pace_sec_per_play_last4_home")),
                },
            },
            "away": {
                "code": away_code,
                "name": TEAM_CODE_TO_FULL.get(away_code, away_code),
                "implied_total": _safe(r.get("away_tt")),
                "coach": r.get("away_coach"),
                "tendencies_last4": {
                    "early_down_pr": _safe(r.get("neutral_early_down_pass_rate_last4_away")),
                    "pace_sec_play": _safe(r.get("neutral_pace_sec_per_play_last4_away")),
                },
            },
            "market": {
                "total": _safe(r.get("total_line")),
                "home_spread": _safe(r.get("home_spread")),
                "totals_book": r.get("totals_book"),
                "spreads_book": r.get("spreads_book"),
            },
            "weather": {
                "temp_f": _safe(r.get("wx_temp_f")),
                "wind_mph": _safe(r.get("wx_wind_mph")),
                "precip_prob_pct": _safe(r.get("wx_precip_prob_pct")),
            },
            "referee": {
                "name": r.get("ref_name"),
                "penalties_per100_last16": _safe(r.get("penalties_per100_last16")),
                "dpi_per100_pass_last16": _safe(r.get("dpi_per100_pass_last16")),
                "off_hold_per100_plays_last16": _safe(r.get("off_hold_per100_plays_last16")),
            }
        })
    return packets

# Strict JSON schema for structured outputs (used when SDK supports response_format)
_JSON_SCHEMA = {
    "name": "picks_schema",
    "strict": True,
    "schema": {
        "type": "object",
        "properties": {
            "picks": {
                "type": "array",
                "items": {
                    "type":"object",
                    "properties":{
                        "game_id":{"type":"string"},
                        "market":{"type":"string","enum":["spread","total","moneyline","pass"]},
                        "selection":{"type":"string"},
                        "confidence":{"type":"number","minimum":0,"maximum":1},
                        "units":{"type":"number","minimum":0},
                        "edge_note":{"type":"string"}
                    },
                    "required":["game_id","market","selection","confidence","units","edge_note"],
                    "additionalProperties": False
                }
            }
        },
        "required": ["picks"],
        "additionalProperties": False
    }
}

_SYSTEM_PICKS = (
    "You are an NFL betting analyst. For each game, either select one market (spread/total/moneyline) "
    "with a clear side/number, or return 'pass'. Use implied totals/spread, weather, coach tendencies (last4), "
    "and referee profile (last16). Keep stakes disciplined. Return ONLY JSON that matches the schema."
)

def llm_picks_via_responses(packets: List[Dict[str,Any]],
                            bankroll_units: float = 10.0,
                            model: str | None = None,
                            chunk_size: int = 6,
                            debug: bool = True) -> pd.DataFrame:
    """Get LLM picks; robust to SDK/model capability differences."""
    mdl = model or os.getenv("OPENAI_MODEL", "gpt-5")
    all_picks, raw_snips = [], []

    for i in range(0, len(packets), chunk_size):
        block = packets[i:i+chunk_size]
        prompt = {"bankroll_units": bankroll_units, "games": block}

        raw = None
        # Preferred: Responses API with strict schema (if SDK supports response_format)
        try:
            resp = client.responses.create(
                model=mdl,
                reasoning={"effort":"low"},
                instructions=_SYSTEM_PICKS,
                input=json.dumps(prompt, ensure_ascii=False),
                response_format={"type":"json_schema", "json_schema": _JSON_SCHEMA},
                max_output_tokens=800,
            )
            raw = (resp.output_text or "").strip()
        except TypeError:
            # Older SDK: retry without response_format, include schema textually
            resp = client.responses.create(
                model=mdl,
                reasoning={"effort":"low"},
                instructions=_SYSTEM_PICKS + " Schema: " + json.dumps(_JSON_SCHEMA["schema"]),
                input=json.dumps(prompt, ensure_ascii=False),
                max_output_tokens=800,
            )
            raw = (resp.output_text or "").strip()
        except Exception:
            raw = None

        parsed = None
        if raw:
            try:
                parsed = json.loads(raw)
            except Exception:
                m = re.search(r"\{.*\}", raw, flags=re.S)
                if m:
                    try:
                        parsed = json.loads(m.group(0))
                    except Exception:
                        parsed = None

        # Fallback: Chat Completions
        if parsed is None or "picks" not in (parsed or {}):
            try:
                chat = client.chat.completions.create(
                    model=mdl,
                    messages=[
                        {"role":"system","content":_SYSTEM_PICKS},
                        {"role":"user","content": json.dumps(prompt, ensure_ascii=False)}
                    ],
                    max_completion_tokens=800,
                    response_format={"type":"json_object"},
                )
                raw = chat.choices[0].message.content.strip()
                parsed = json.loads(raw)
            except Exception:
                chat = client.chat.completions.create(
                    model=mdl,
                    messages=[
                        {"role":"system","content":_SYSTEM_PICKS + " Output JSON only."},
                        {"role":"user","content": json.dumps(prompt, ensure_ascii=False)}
                    ],
                    max_completion_tokens=800,
                )
                raw = chat.choices[0].message.content.strip()
                try:
                    parsed = json.loads(raw)
                except Exception:
                    parsed = {"picks":[]}

        if debug and raw:
            raw_snips.append(raw[:300])

        picks = (parsed or {}).get("picks", [])
        for p in picks:
            if {"game_id","market","selection"}.issubset(p):
                p["confidence"] = float(p.get("confidence", 0) or 0)
                p["units"] = float(p.get("units", 0) or 0)
                p["edge_note"] = str(p.get("edge_note","") or "")
                all_picks.append(p)

    df = pd.DataFrame(all_picks)
    if df.empty and debug:
        print("LLM returned no picks. Raw snippet(s) for debugging:")
        for k, snip in enumerate(raw_snips, 1):
            print(f"--- Chunk {k} ---\n{snip}\n")

    if df.empty:
        return df

    order = ["game_id","market","selection","confidence","units","edge_note"]
    order = [c for c in order if c in df.columns]
    return df[order].sort_values(["confidence","units"], ascending=[False,False]).reset_index(drop=True)

# ----- RUN: get picks -----
packets = build_game_packets(bundle)
print("Packets:", len(packets))
picks_df = llm_picks_via_responses(
    packets,
    bankroll_units=10.0,
    model=os.getenv("OPENAI_MODEL","gpt-5"),
    debug=True
)
display(picks_df.head(15))


# =========================
# 14) Assistant Q&A over this week's bundle
# =========================
def _find_games_by_text(question: str, sched_df: pd.DataFrame) -> list[str]:
    q = question.lower()
    want = set()
    for code, full in TEAM_CODE_TO_FULL.items():
        if (re.search(rf'\b{code.lower()}\b', q) or re.search(rf'\b{full.lower()}\b', q)):
            want.add(code)
    if not want:
        return sched_df["game_id"].tolist()
    mask = sched_df["home_team"].isin(want) | sched_df["away_team"].isin(want)
    return sched_df.loc[mask, "game_id"].tolist()

def _limit_context_rows(df: pd.DataFrame, n: int = 8) -> pd.DataFrame:
    return df.head(n).copy()

def build_qa_context(bundle: dict, question: str) -> dict:
    imp   = build_implied_totals(bundle, strict_fanduel=False)
    wx    = expand_weather(bundle)[["home_team","kickoff","wx_temp_f","wx_wind_mph","wx_precip_prob_pct"]]
    coach = bundle.get("coach_ctx", pd.DataFrame())
    refc  = bundle.get("ref_ctx", pd.DataFrame())
    sched = bundle["schedule"][["game_id","season","week","home_team","away_team","kickoff"]] \
              .rename(columns={"home_team":"home_code","away_team":"away_code"})

    game_ids = _find_games_by_text(question, bundle["schedule"])
    sched = sched[sched["game_id"].isin(game_ids)].copy()

    pref = ["game_id","season","week","kickoff","home_code","away_code"]
    keys = [k for k in pref if k in imp.columns and k in sched.columns] or ["game_id"]
    base = sched.merge(imp, on=keys, how="left")

    if {"home_code","kickoff"}.issubset(base.columns):
        base = base.merge(wx.rename(columns={"home_team":"home_code"}), on=["home_code","kickoff"], how="left")
    else:
        base = base.merge(wx.rename(columns={"home_team":"home_code"}).drop(columns=["kickoff"], errors="ignore"),
                          on="home_code", how="left")

    if not coach.empty:
        last4 = [c for c in coach.columns if c.endswith("_last4")]
        h = coach[coach["is_home"]==True ][["game_id","team","coach_name"]+last4] \
               .rename(columns={"team":"home_code","coach_name":"home_coach"})
        a = coach[coach["is_home"]==False][["game_id","team","coach_name"]+last4] \
               .rename(columns={"team":"away_code","coach_name":"away_coach"})
        for c in last4:
            h.rename(columns={c:f"{c}_home"}, inplace=True)
            a.rename(columns={c:f"{c}_away"}, inplace=True)
        base = base.merge(h, on=["game_id","home_code"], how="left") \
                   .merge(a, on=["game_id","away_code"], how="left")

    if not refc.empty:
        rkeep  = ["game_id","ref_name","penalties_per100_last16","dpi_per100_pass_last16","off_hold_per100_plays_last16"]
        rsmall = refc[[c for c in rkeep if c in refc.columns]].drop_duplicates("game_id")
        base   = base.merge(rsmall, on="game_id", how="left")

    keep = [c for c in [
        "game_id","season","week","kickoff",
        "home_code","away_code","home_full","away_full",
        "total_line","home_spread","home_tt","away_tt",
        "wx_temp_f","wx_wind_mph","wx_precip_prob_pct",
        "home_coach","away_coach",
        "neutral_early_down_pass_rate_last4_home","neutral_pace_sec_per_play_last4_home",
        "neutral_early_down_pass_rate_last4_away","neutral_pace_sec_per_play_last4_away",
        "ref_name","penalties_per100_last16","dpi_per100_pass_last16","off_hold_per100_plays_last16"
    ] if c in base.columns]
    ctx = _limit_context_rows(base[keep], n=8).copy()

    # ensure full names exist, then convert to pure Python via JSON (ISO dates)
    if "home_full" not in ctx.columns:
        ctx["home_full"] = ctx["home_code"].map(TEAM_CODE_TO_FULL)
    if "away_full" not in ctx.columns:
        ctx["away_full"] = ctx["away_code"].map(TEAM_CODE_TO_FULL)

    games = json.loads(ctx.to_json(orient="records", date_format="iso"))
    return {"question": question, "games": games}

def ask_ai(bundle: dict, question: str, model: Optional[str] = None, max_output_tokens: int = 800) -> str:
    """Ask a natural-language question about the current week. Returns plain text."""
    mdl = model or os.getenv("OPENAI_MODEL", "gpt-5")
    context = build_qa_context(bundle, question)
    system  = (
        "You are an NFL data assistant. Use ONLY the JSON context provided. "
        "If data is missing, say so briefly. Keep answers concise and actionable."
    )
    resp = client.responses.create(
        model=mdl,
        reasoning={"effort":"low"},
        instructions=system,
        input=json.dumps(context, ensure_ascii=False),
        max_output_tokens=max_output_tokens,
    )
    return (resp.output_text or "").strip()

# ----- RUN: Q&A smoke tests -----
print(ask_ai(bundle,
    "Which three games this week look most weather-affected or pace-skewed? Bullet the games with totals/spreads and a one-line note."))

gid = bundle['schedule']['game_id'].iloc[0]
home = bundle['schedule'].loc[bundle['schedule']['game_id']==gid, 'home_team'].iloc[0]
away = bundle['schedule'].loc[bundle['schedule']['game_id']==gid, 'away_team'].iloc[0]
print(ask_ai(bundle,
    f"Scouting card for {TEAM_CODE_TO_FULL[home]} vs {TEAM_CODE_TO_FULL[away]}: pace, pass tilt, weather, ref angle, lean."))